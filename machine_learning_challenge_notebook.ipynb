{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "from gensim import utils\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data \n",
    "twenty_train = fetch_20newsgroups(subset='train', random_state = 42,shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "twenty_test = fetch_20newsgroups(subset='test', random_state = 42, shuffle=True, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:  Explanatory Data Analysis#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categories names are:\\n\")\n",
    "for i in twenty_train.target_names:\n",
    "    print(i)\n",
    "print(\"\\n Total:{}\".format(len(twenty_train.target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of the training data set is {}\".format(len(twenty_train.data))) \n",
    "print(\"Length of the test data set is {}\".format(len(twenty_test.data))) \n",
    "print(\"Length of the total dataset is {}\".format(11314+7532))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "test_names = []\n",
    "for i in twenty_train.target:\n",
    "    names.append(twenty_train.target_names[i])\n",
    "for i in twenty_test.target:\n",
    "    test_names.append(twenty_test.target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting dataset into a pandas dataframe\n",
    "df_train = pd.DataFrame(data = np.c_[twenty_train.data,twenty_train.target,names] , columns = (\"Text\",\"Target_id\",\"Target_name\")) \n",
    "df_test  = pd.DataFrame(data = np.c_[twenty_test.data,twenty_test.target,test_names] , columns = (\"Text\",\"Target\",\"Target_name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are the classes balanced? \n",
    "count_target = df_train['Target_id'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.barplot(count_target.index,  count_target.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot shows that the classes are little unbalanced. News reports from categories 'comp.graphics', 'talk.politics.misc' & 'talk.religion.misc' are fewer compared to other categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the properties by target#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating extra features \n",
    "def add_features(df):\n",
    "    df['Length_of_text'] = df['Text'].apply(lambda x: len(str(x))) # length of each text\n",
    "    df['unique'] = df['Text'].apply(lambda x: len(set(str(x)))) # Unique characters\n",
    "    df['punctuations'] = df['Text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    df['uniq_punctuations'] = df['Text'].apply(lambda x: len(set([c for c in str(x) if c in string.punctuation])))\n",
    "    df['letters'] = df['Text'].apply(lambda x: len([c for c in str(x) if c.isalpha()])) \n",
    "    df['uniq_letters'] = df['Text'].apply(lambda x: len(set([c for c in str(x) if c.isalpha()])))\n",
    "    df['numbers'] = df['Text'].apply(lambda x: len([c for c in str(x) if c.isdigit()]))\n",
    "    df['uniq_numbers'] = df['Text'].apply(lambda x: len(set([c for c in str(x) if c.isdigit()])))\n",
    "    df['uppercase'] = df['Text'].apply(lambda x: len([c for c in str(x) if c.isupper()]))\n",
    "    df['uniq_uppercase'] = df['Text'].apply(lambda x: len(set([c for c in str(x) if c.isupper()])))\n",
    "    df['lowercase'] = df['Text'].apply(lambda x: len([c for c in str(x) if c.islower()]))\n",
    "    df['uniq_lowercase'] = df['Text'].apply(lambda x: len(set([c for c in str(x) if c.islower()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying features on training and test data\n",
    "add_features(df_train)\n",
    "add_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exploring each feature\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.catplot(x='Target_id', data=df_train,kind=\"count\")\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.ylabel('Length of text', fontsize=12)\n",
    "plt.title(\"Length of text by target\", fontsize=15)\n",
    "plt.show()\n",
    "###########\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.violinplot(x='Target_id', y='unique', data=df_train)\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.ylabel('Number of unique characters in text', fontsize=12)\n",
    "plt.title(\"Number of unique characters by target\", fontsize=15)\n",
    "plt.show()\n",
    "###########\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.violinplot(x='Target_id', y='uniq_punctuations', data=df_train)\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.ylabel('Number of unique punctuations in text', fontsize=12)\n",
    "plt.title(\"Number of unique punctuations by target\", fontsize=15)\n",
    "plt.show()\n",
    "###########\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.violinplot(x='Target_id', y='uniq_letters', data=df_train)\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.ylabel('Number of unique letters in text', fontsize=12)\n",
    "plt.title(\"Number of unique letters by target\", fontsize=15)\n",
    "plt.show()\n",
    "###########\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.violinplot(x='Target_id', y='uniq_numbers', data=df_train)\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.ylabel('Number of unique characters in text', fontsize=12)\n",
    "plt.title(\"Number of unique characters by target\", fontsize=15)\n",
    "plt.show()\n",
    "###########\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.violinplot(x='Target_id', y='uniq_uppercase', data=df_train)\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.ylabel('Number of unique uppercase in text', fontsize=12)\n",
    "plt.title(\"Number of unique uppercase by target\", fontsize=15)\n",
    "plt.show()\n",
    "###########\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.violinplot(x='Target_id', y='uniq_lowercase', data=df_train)\n",
    "plt.xlabel('Target_id', fontsize=12)\n",
    "plt.ylabel('Number of unique lowercase in text', fontsize=12)\n",
    "plt.title(\"Number of unique lowercase by target\", fontsize=15)\n",
    "plt.show()\n",
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing first post to explore \\n\")\n",
    "print(\"\\n\".join(df_train['Text'][0].split(\"\\n\")[:])) #prints first line of the first data file\n",
    "print(\"\\n Above post belongs to {}\".format(df_train.Target_name[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create filters to clean the text\n",
    "filters = [\n",
    "           gsp.strip_tags,  #Unicode string without tags.\n",
    "           gsp.strip_punctuation, #Unicode string without punctuation characters.\n",
    "           gsp.strip_multiple_whitespaces, #Unicode string without repeating in a row whitespace characters.\n",
    "           gsp.strip_numeric, #Unicode string without digits.\n",
    "           gsp.remove_stopwords, #Unicode string without STOPWORDS.\n",
    "           gsp.strip_short, #Unicode string without short words.\n",
    "           #gsp.stem_text #Unicode lowercased and porter-stemmed version of string text.\n",
    "          ]\n",
    "\n",
    "#fucntion to clean text\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "#cleaning training and test dataset\n",
    "df_train['Text'] = df_train['Text'].apply(clean_text)\n",
    "df_test['Text'] = df_test['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#Exploring the cleaned dataset\n",
    "########################\n",
    "#Counting number of features in the new cleaned dataset\n",
    "vectorizer = TfidfVectorizer() #vectorizer\n",
    "vectors = vectorizer.fit_transform(df_train.Text)\n",
    "clf = MultinomialNB(alpha=.01) #classifier\n",
    "clf.fit(vectors, twenty_train.target)\n",
    "print(\"Number of features in the datset are {} \\n \\n\".format(np.shape(vectors)[1])) \n",
    "######################\n",
    "#creating function to display some features for each category\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"{}: {}\".format(category, \" \".join(feature_names[top10])))\n",
    "#########################\n",
    "print(\"Display first 10 features for each category \\n\")\n",
    "show_top10(clf, vectorizer, twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, all the features are only words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data using word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "def plot_word_cloud(text):\n",
    "    wordcloud_instance = WordCloud(width = 800, height = 800, \n",
    "                background_color ='black', \n",
    "                stopwords='english',\n",
    "                min_font_size = 5).generate(text) \n",
    "             \n",
    "    plt.figure(figsize = (10, 10), facecolor = None) \n",
    "    plt.imshow(wordcloud_instance) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "def plot_word_cloud_for_cat(category):\n",
    "    text_df = df_train.loc[df_train['Target_name'] == str(category)]\n",
    "    texts = ''\n",
    "    for index, item in text_df.iterrows():\n",
    "        texts = texts + ' ' + clean_text(item['Text'])\n",
    "    plot_word_cloud(texts)\n",
    "\n",
    "#plotting word cloud for one category\n",
    "plot_word_cloud_for_cat('comp.graphics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Testing different classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-nominal Naive bayesian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi-nominal Naive bayesian classifier\n",
    "#making pipeline\n",
    "mulit_nb_clf = Pipeline([\n",
    "      ('vect', CountVectorizer(stop_words='english')),\n",
    "      ('tfidf', TfidfTransformer()),\n",
    "      ('clf', MultinomialNB(fit_prior=False)), \n",
    "            ])\n",
    "#creating an array of possible parameters\n",
    "parameters = {'vect__max_df': (0.5,0.75,1.0),\n",
    "              'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "#gridding over all the parameters\n",
    "gs_clf = GridSearchCV(mulit_nb_clf, parameters,n_jobs=-1)\n",
    "\n",
    "#fitting the model\n",
    "gs_clf = gs_clf.fit(df_train.Text,df_train.Target_id) \n",
    "\n",
    "#name of the file used for saving the model \n",
    "filename = 'mulit_nb_clf.sav' \n",
    "# save the model to disk\n",
    "#pickle.dump(gs_clf, open(filename, 'wb')) #uncomment to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model from disk\n",
    "gs_clf = pickle.load(open(filename, 'rb'))\n",
    "#predict the categories\n",
    "predicted_mulit_nb = gs_clf.predict(df_test.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross validation accuracy using Naive Bayes Classifier on training dataset, with help of gridsearch is {} %\".format(gs_clf.best_score_*100)) \n",
    "print(\"Accuracy using Naive Bayes Classifier on test dataset, with help of gridsearch is {} %\".format(np.mean(predicted == df_test.Target)*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear SVM\n",
    "#making pipeline\n",
    "sgd_clf = Pipeline([\n",
    "      ('vect', CountVectorizer(stop_words='english')),\n",
    "      ('tfidf', TfidfTransformer()),\n",
    "      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',random_state=42)), \n",
    "            ])\n",
    "#creating an array of possible parameters\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf-svm__alpha': (1e-2, 1e-3)}\n",
    "\n",
    "#gridding over all the parameters\n",
    "sgd_clf = GridSearchCV(sgd_clf, parameters,n_jobs=-1)\n",
    "\n",
    "#fitting the model\n",
    "sgd_clf = sgd_clf.fit(df_train.Text,df_train.Target_id)\n",
    "\n",
    "filename = 'sgd_clf.sav'\n",
    "#pickle.dump(sgd_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sgd_clf.sav'\n",
    "# load the model from disk\n",
    "sgd_clf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#predict the categories\n",
    "predicted = sgd_clf.predict(df_test.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross validation accuracy using SGDClassifier on training dataset, with help of gridsearch is {} %\".format(sgd_clf.best_score_*100)) \n",
    "print(\"Accuracy using SGDClassifier on test dataset, with help of gridsearch is {} %\".format(np.mean(predicted == df_test.Target)*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###SVC\n",
    "#making pipeline\n",
    "svc_clf = Pipeline([\n",
    "      ('vect', CountVectorizer(stop_words='english')),\n",
    "      ('tfidf', TfidfTransformer()),\n",
    "      ('svc_clf', svm.SVC()), \n",
    "            ])\n",
    "\n",
    "#creating an array of possible parameters\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False)}\n",
    "\n",
    "#gridding over all the parameters\n",
    "svc_clf = GridSearchCV(svc_clf, parameters,n_jobs=-1)\n",
    "\n",
    "#fitting the model\n",
    "svc_clf = svc_clf.fit(df_train.Text,df_train.Target_id)\n",
    "\n",
    "filename = 'svc_clf.sav'\n",
    "#pickle.dump(svc_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'svc_clf.sav'\n",
    "# load the model from disk\n",
    "svc_clf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#predict the categories\n",
    "predicted = svc_clf.predict(df_test.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross validation accuracy using SVC Classifier on training dataset, with help of gridsearch is {} %\".format(svc_clf.best_score_*100)) \n",
    "print(\"Accuracy using SVC Classifier on test dataset, with help of gridsearch is {} %\".format(np.mean(predicted == df_test.Target)*100)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Random forest\n",
    "#making pipeline\n",
    "rf_clf = Pipeline([\n",
    "      ('vect', CountVectorizer(stop_words='english')),\n",
    "      ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "      ('clf_rf', RandomForestClassifier(n_estimators=100, \n",
    "                               random_state=46, \n",
    "                               max_features = 'sqrt',\n",
    "                               n_jobs=-1, verbose = 1)), \n",
    "            ])\n",
    "\n",
    "#creating an array of possible parameters\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)]}\n",
    "\n",
    "#gridding over all the parameters\n",
    "rf_clf = GridSearchCV(rf_clf, parameters,n_jobs=-1)\n",
    "\n",
    "#fitting the model\n",
    "rf_clf = rf_clf.fit(df_train.Text,df_train.Target_id)\n",
    "\n",
    "filename = 'rf_clf.sav'\n",
    "#pickle.dump(rf_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model from disk\n",
    "rf_clf = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#predict the categories\n",
    "predicted = rf_clf.predict(df_test.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cross validation accuracy using Random forest Classifier on training dataset, with help of gridsearch is {} %\".format(rf_clf.best_score_*100)) \n",
    "print(\"Accuracy using Random forest Classifier on test dataset, with help of gridsearch is {} %\".format(np.mean(predicted == df_test.Target)*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing classification report\n",
    "print(classification_report(df_test.Target, predicted_mulit_nb, target_names=twenty_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the confusion matrix\n",
    "cm = metrics.confusion_matrix(df_test.Target, predicted_mulit_nb)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, fmt='d',\n",
    "            xticklabels=twenty_train.target_names, yticklabels=twenty_train.target_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_word_cloud_for_cat('rec.sport.hockey') \n",
    "# plot_word_cloud_for_cat('rec.motorcycles') "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
